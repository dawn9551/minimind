# 12 - 前向传播与损失计算详解

## 一、模块概述

在 `train_epoch()` 函数中（Line 26-43），每个训练步骤的核心是前向传播和损失计算：

```python
for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
    # 1. 数据移到GPU
    X = X.to(args.device)
    Y = Y.to(args.device)
    loss_mask = loss_mask.to(args.device)
    
    # 2. 动态学习率
    lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    
    # 3. 前向传播与损失计算
    with autocast_ctx:
        res = model(X)
        loss = loss_fct(
            res.logits.view(-1, res.logits.size(-1)),
            Y.view(-1)
        ).view(Y.size())
        
        loss = (loss * loss_mask).sum() / loss_mask.sum()
        loss += res.aux_loss
        loss = loss / args.accumulation_steps
```

**核心步骤：**
1. 数据移到GPU
2. 动态调整学习率
3. 模型前向传播
4. 计算CrossEntropyLoss
5. 应用loss mask
6. 添加辅助损失（MoE）
7. 梯度累积缩放

---

## 二、数据准备

### 2.1 从DataLoader获取数据

```python
# Line 26
for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
```

**数据形状：**

| 张量 | 形状 | 说明 |
|------|------|------|
| **X** | [batch, seq_len-1] | 输入序列（input_ids[:-1]） |
| **Y** | [batch, seq_len-1] | 标签序列（input_ids[1:]） |
| **loss_mask** | [batch, seq_len-1] | 损失掩码（只计算assistant） |

**示例（batch_size=2, seq_len=340）：**
```python
X.shape = torch.Size([2, 339])       # 输入前339个token
Y.shape = torch.Size([2, 339])       # 标签后339个token
loss_mask.shape = torch.Size([2, 339])  # 掩码（0或1）
```

**enumerate的start参数：**
```python
# start=start_step + 1
# 正常训练：start=0+1=1 → step从1开始
# 断点续训（start_step=300）：start=300+1=301 → step从301开始

# 因此step始终从1开始计数（不是0）
```

---

### 2.2 数据移到GPU

```python
# Line 27-29
X = X.to(args.device)
Y = Y.to(args.device)
loss_mask = loss_mask.to(args.device)
```

**为什么要移到GPU？**
- DataLoader返回的张量在CPU上（pin_memory可选）
- 模型在GPU上，计算必须在同一设备

**args.device 的值：**
```python
# 单GPU/CPU
args.device = "cuda:0" 或 "cpu"

# 多GPU（DDP）
args.device = f"cuda:{local_rank}"
# 进程0: "cuda:0"
# 进程1: "cuda:1"
```

**性能优化：**
```python
# pin_memory=True时，传输速度更快
loader = DataLoader(..., pin_memory=True)

# CPU → GPU传输是异步的，不会阻塞训练\
```

---

## 三、动态学习率调度

### 3.1 计算当前学习率

```python
# Line 30
lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)
```

**get_lr() 函数（trainer_utils.py）：**
```python
def get_lr(current_step, total_steps, lr):
    return lr * (0.1 + 0.45 * (1 + math.cos(math.pi * current_step / total_steps)))
```

**参数计算：**
```python
# current_step: 当前总步数
current_step = epoch * iters + step

# 示例：
# Epoch 0, step 100, iters=500
# current_step = 0 * 500 + 100 = 100

# Epoch 1, step 200, iters=500
# current_step = 1 * 500 + 200 = 700

# total_steps: 总训练步数
total_steps = args.epochs * iters

# 示例：epochs=2, iters=500
# total_steps = 2 * 500 = 1000
```

**学习率曲线（Cosine衰减）：**
```
lr
 ↑
1.0| *
0.9|  *
0.8|   **
0.7|     **
0.6|       ***
0.5|          ****
0.4|              ****
0.3|                  *****
0.2|                        ******
0.1|                              ********_____
   |________________________________________→ step
   0       250      500      750      1000
```

**公式解析：**
```python
# Cosine衰减
ratio = math.cos(math.pi * current_step / total_steps)
# step=0:    cos(0) = 1
# step=500:  cos(π/2) = 0
# step=1000: cos(π) = -1

# 归一化到[0.1, 1.0]
scale = 0.1 + 0.45 * (1 + ratio)
# ratio=1:  0.1 + 0.45 * 2 = 1.0（初始）
# ratio=0:  0.1 + 0.45 * 1 = 0.55（中间）
# ratio=-1: 0.1 + 0.45 * 0 = 0.1（最终）

# 最终学习率
lr_t = lr_initial * scale
```

---

### 3.2 更新优化器学习率

```python
# Line 31-32
for param_group in optimizer.param_groups:
    param_group['lr'] = lr
```

**optimizer.param_groups 结构：**
```python
optimizer.param_groups = [
    {
        'params': [param1, param2, ...],  # 参数列表
        'lr': 5e-7,                        # 学习率
        'betas': (0.9, 0.999),
        'eps': 1e-08,
        'weight_decay': 0.01,
        ...
    }
]
```

**为什么遍历param_groups？**
- 支持不同参数组使用不同学习率
- 本项目只有一个param_group
- 遍历是通用做法

**动态调整的优势：**
- 无需手动创建lr_scheduler
- 每步都更新（精细控制）
- 适合Cosine衰减

---

## 四、前向传播

### 4.1 使用混合精度

```python
# Line 34
with autocast_ctx:
    res = model(X)
    ...
```

**autocast_ctx 回顾（文档05）：**
```python
# GPU训练
autocast_ctx = torch.cuda.amp.autocast(dtype=torch.bfloat16)

# CPU训练
autocast_ctx = nullcontext()
```

**作用：**
- 自动选择精度（BF16/FP16/FP32）
- 加速训练，节省显存

---

### 4.2 模型前向传播

```python
# Line 35
res = model(X)
```

**输入与输出：**
```python
# 输入
X: [batch, seq_len-1] = [2, 339]

# 输出（res是CausalLMOutputWithPast对象）
res.logits: [batch, seq_len-1, vocab_size] = [2, 339, 6400]
res.aux_loss: scalar（MoE辅助损失，非MoE为0）
res.past_key_values: None（训练时不使用）
```

**logits 含义：**
```python
# logits[i, j, k] = 第i个样本，第j个位置，预测第k个token的得分（未归一化）

# 示例：
logits[0, 0, :].shape = [6400]  # 第0个样本，第0个位置，对所有6400个token的预测得分
# [3.2, -1.5, 0.8, ..., 2.1]  # 得分越高，越可能是下一个token
```

---

## 五、损失计算

### 5.1 CrossEntropyLoss

```python
# Line 24（函数开始时定义）
loss_fct = nn.CrossEntropyLoss(reduction='none')

# Line 36-39
loss = loss_fct(
    res.logits.view(-1, res.logits.size(-1)),
    Y.view(-1)
).view(Y.size())
```

**CrossEntropyLoss 公式：**
```python
# 对于单个样本的单个位置：
loss = -log(softmax(logits)[target])
     = -log(exp(logits[target]) / sum(exp(logits)))
```

**reduction='none' 的作用：**
```python
# reduction='mean': 返回标量（平均loss）
# reduction='sum':  返回标量（总loss）
# reduction='none': 返回每个位置的loss（用于loss mask）

# 示例：
logits: [2, 339, 6400] → flatten → [678, 6400]
Y: [2, 339] → flatten → [678]
loss: [678]  # 每个位置的loss
```

---

### 5.2 view操作详解

```python
# 展平为2D
logits_2d = res.logits.view(-1, res.logits.size(-1))
# [2, 339, 6400] → [678, 6400]

Y_1d = Y.view(-1)
# [2, 339] → [678]

# 计算loss
loss_1d = loss_fct(logits_2d, Y_1d)
# [678]

# 恢复形状
loss_2d = loss_1d.view(Y.size())
# [678] → [2, 339]
```

**为什么要view？**
- `CrossEntropyLoss` 要求输入为 `[N, C]` 和 `[N]`
- `N`=样本数，`C`=类别数（vocab_size）
- 需要将 `[batch, seq, vocab]` 展平为 `[batch*seq, vocab]`

---

### 5.3 应用loss mask

```python
# Line 41
loss = (loss * loss_mask).sum() / loss_mask.sum()
```

**逐步计算：**

```python
# loss: [batch, seq_len-1] = [2, 339]
# loss_mask: [batch, seq_len-1] = [2, 339]

# 示例数据：
loss = [[1.2, 0.8, 1.5, ...],  # 样本0的所有位置loss
        [0.9, 1.1, 1.3, ...]]  # 样本1的所有位置loss

loss_mask = [[0, 0, 0, 1, 1, 1, ...],  # 样本0：前3个位置不计算（user输入）
             [0, 0, 1, 1, 1, 1, ...]]  # 样本1：前2个位置不计算

# 逐元素相乘（只保留mask=1的位置）
masked_loss = loss * loss_mask
= [[0,   0,   0,   1.5, ...],
   [0,   0,   1.3, 1.3, ...]]

# 求和（只对assistant回复求和）
sum_loss = masked_loss.sum()  # 所有mask=1位置的loss总和

# 归一化（除以有效位置数）
num_valid = loss_mask.sum()   # mask=1的总数
final_loss = sum_loss / num_valid
```

**为什么这样计算？**
1. **只监督assistant回复**：忽略user输入的loss
2. **平均损失**：除以有效位置数，得到每个位置的平均loss
3. **batch之间平衡**：不同batch的有效位置数可能不同

---

### 5.4 添加辅助损失（MoE）

```python
# Line 42
loss += res.aux_loss
```

**aux_loss 详解：**

```python
# 非MoE模型
res.aux_loss = 0.0  # 无辅助损失

# MoE模型
res.aux_loss = load_balancing_loss  # 平衡专家负载

# 公式（简化）：
aux_loss = sum((fi - 1/n_experts)^2 for all experts)
# fi: 第i个专家的选择频率
# 目标：让每个专家被平均选择
```

**为什么需要辅助损失？**
```python
# 问题：MoE训练中，可能所有token都选择同一个专家
# Expert 0: 100% token选择  ← 过载
# Expert 1: 0% token选择    ← 闲置
# Expert 2: 0% token选择
# Expert 3: 0% token选择

# 辅助损失鼓励负载均衡
# Expert 0: 25% token
# Expert 1: 25% token
# Expert 2: 25% token
# Expert 3: 25% token
```

---

### 5.5 梯度累积缩放

```python
# Line 43
loss = loss / args.accumulation_steps
```

**为什么要除以accumulation_steps？**

```python
# 梯度累积原理：
# 1. 将大batch拆分为多个小batch
# 2. 每个小batch计算loss并backward
# 3. 梯度会累加
# 4. 多个小batch后，统一更新参数

# 问题：
# - loss.backward()会将loss的梯度累加
# - 如果不缩放，累加的梯度会是原来的N倍

# 示例：
accumulation_steps = 4

# Micro-batch 1
loss1 = 2.0
loss1 = loss1 / 4 = 0.5
loss1.backward()  # 梯度 = ∇loss1 / 4

# Micro-batch 2
loss2 = 1.8
loss2 = loss2 / 4 = 0.45
loss2.backward()  # 梯度累加 = (∇loss1 + ∇loss2) / 4

# ... 

# Micro-batch 4
loss4 = 2.1
loss4 = loss4 / 4 = 0.525
loss4.backward()  # 梯度累加 = (∇loss1 + ∇loss2 + ∇loss3 + ∇loss4) / 4

# 最终梯度 = 平均梯度（等效于batch_size*4的梯度）
```

---

## 六、完整计算流程

```mermaid
graph TD
    A[获取数据<br/>X, Y, loss_mask] --> B[移到GPU]
    B --> C[动态学习率]
    C --> D[前向传播<br/>model]
    D --> E[logits<br/>[batch, seq, vocab]]
    E --> F[CrossEntropyLoss<br/>reduction='none']
    F --> G[loss<br/>[batch, seq]]
    G --> H[应用loss mask<br/>只计算assistant]
    H --> I[归一化<br/>除以有效位置数]
    I --> J[添加aux_loss<br/>MoE专家负载]
    J --> K[梯度累积缩放<br/>除以accumulation_steps]
    K --> L[最终loss<br/>标量]
```

---

## 七、实际数值示例

### 7.1 完整计算过程

```python
# 假设：
batch_size = 2
seq_len = 340
vocab_size = 6400
accumulation_steps = 1

# 输入
X = torch.tensor([[151644, 872, ...], [...]])  # [2, 339]
Y = torch.tensor([[872, 105135, ...], [...]])  # [2, 339]
loss_mask = torch.tensor([[0, 0, 1, 1, ...], [0, 1, 1, ...]])  # [2, 339]

# 前向传播
res = model(X)
# res.logits: [2, 339, 6400]

# CrossEntropyLoss
loss = loss_fct(res.logits.view(-1, 6400), Y.view(-1))
# loss: [678]（每个位置的loss）

# 恢复形状
loss = loss.view(2, 339)
# loss: [[1.234, 0.987, 2.345, ...],
#        [1.567, 1.234, 2.012, ...]]

# 应用loss mask
masked_loss = loss * loss_mask
# [[0, 0, 2.345, ...],  # 前2个位置忽略
#  [0, 1.234, 2.012, ...]]  # 前1个位置忽略

# 归一化
total_loss = masked_loss.sum()  # 假设 = 500.234
num_valid = loss_mask.sum()     # 假设 = 300
avg_loss = total_loss / num_valid  # 500.234 / 300 = 1.667

# 添加aux_loss（非MoE为0）
avg_loss += 0.0

# 梯度累积缩放
final_loss = avg_loss / 1  # 1.667

# 输出
print(f"loss: {final_loss:.6f}")  # loss: 1.667000
```

---

## 八、常见问题

### Q1: 为什么loss突然变成NaN？

**可能原因：**

```python
# 1. 学习率过大导致梯度爆炸
lr = 1e-3  # 太大！SFT推荐5e-7

# 2. loss_mask全为0（无有效位置）
loss = sum_loss / loss_mask.sum()  # 除以0！

# 3. 数据问题（无效token ID）
Y = [151644, 99999999, ...]  # 超出vocab_size
```

**解决方案：**
```python
# 1. 降低学习率
--learning_rate 5e-7

# 2. 检查数据（确保有assistant回复）
# 3. 添加数值稳定性
loss = sum_loss / (loss_mask.sum() + 1e-8)
```

---

### Q2: 梯度累积如何影响batch_size？

**等效batch_size：**
```python
# 设置
per_gpu_batch_size = 16
accumulation_steps = 4
num_gpus = 2

# 等效batch_size
effective_batch_size = per_gpu_batch_size * accumulation_steps * num_gpus
                     = 16 * 4 * 2
                     = 128

# 每次optimizer.step()相当于处理了128个样本
```

---

### Q3: 如何查看每个位置的loss？

**调试代码：**
```python
# 在train_epoch中添加
if step == 1:  # 只打印第一步
    # 计算每个位置的loss
    loss_per_token = loss_fct(
        res.logits.view(-1, res.logits.size(-1)),
        Y.view(-1)
    ).view(Y.size())
    
    # 打印前10个位置
    print("Loss per position (first 10):")
    print(loss_per_token[0, :10])
    print("Mask (first 10):")
    print(loss_mask[0, :10])
```

---

## 九、知识点总结

### 核心概念

| 概念 | 说明 |
|------|------|
| **CrossEntropyLoss** | 分类损失函数，用于语言模型 |
| **reduction='none'** | 返回每个位置的loss（用于loss mask） |
| **loss mask** | 只计算assistant回复的loss |
| **aux_loss** | MoE辅助损失，平衡专家负载 |
| **梯度累积缩放** | loss除以accumulation_steps |
| **Cosine学习率** | 从初始值平滑衰减到10% |

### 关键代码

```python
# 前向传播与损失计算
with autocast_ctx:
    res = model(X)
    loss = loss_fct(res.logits.view(-1, vocab_size), Y.view(-1)).view(Y.size())
    loss = (loss * loss_mask).sum() / loss_mask.sum()  # 只计算assistant
    loss += res.aux_loss  # MoE辅助损失
    loss = loss / args.accumulation_steps  # 梯度累积缩放
```

---

## 十、下一步学习内容

在下一节中，我们将深入分析：

**反向传播与参数更新**
1. 梯度缩放与反向传播
2. 梯度累积机制
3. 梯度裁剪
4. 优化器更新
5. 模型保存与日志记录

---

**状态：** ✅ 已完成前向传播与损失计算深度分析
